{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPytmGZ8JWBH2bOvvCUer5S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spate472/RecreatingRetinaUNet/blob/main/Attempt2_ToyDataPyHealth_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYTILg8EW-qj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm # Use notebook version for Colab\n",
        "import math\n",
        "\n",
        "# --- Configuration ---\n",
        "IMAGE_SIZE = 320\n",
        "SPLITS = {\n",
        "    \"train\": 1000,\n",
        "    \"val\": 500,\n",
        "    \"test\": 1000,\n",
        "}\n",
        "# Total 2500 images per task\n",
        "\n",
        "# Object properties\n",
        "OBJECT_INTENSITY = 0.2\n",
        "NOISE_AMPLITUDE = 0.1 # Uniform noise range [-0.1, 0.1]\n",
        "\n",
        "# Task 1 & 2 specific parameters\n",
        "CIRCLE_RADIUS_T12 = 15\n",
        "DONUT_OUTER_RADIUS_T12 = 15\n",
        "DONUT_INNER_RADIUS_T12 = 7\n",
        "\n",
        "# Task 3 specific parameters\n",
        "CIRCLE_RADIUS_T3_CLS1 = 9 # Corresponds to diameter ~19\n",
        "CIRCLE_RADIUS_T3_CLS2 = 10 # Corresponds to diameter 20\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def create_base_image(size):\n",
        "    \"\"\"Creates a black base image (numpy array).\"\"\"\n",
        "    return np.zeros((size, size), dtype=np.float32)\n",
        "\n",
        "def add_noise(image_array):\n",
        "    \"\"\"Adds uniform noise and clips values to [0, 1].\"\"\"\n",
        "    noise = np.random.uniform(-NOISE_AMPLITUDE, NOISE_AMPLITUDE, image_array.shape)\n",
        "    noisy_image = image_array + noise\n",
        "    return np.clip(noisy_image, 0.0, 1.0)\n",
        "\n",
        "def draw_circle(image, center, radius, color, thickness=-1):\n",
        "    \"\"\"Draws a filled circle using cv2.\"\"\"\n",
        "    cv2.circle(image, center, radius, color, thickness)\n",
        "\n",
        "def draw_donut(image, center, outer_radius, inner_radius, color):\n",
        "    \"\"\"Draws a donut (filled circle with a smaller black circle).\"\"\"\n",
        "    # Draw the outer circle\n",
        "    cv2.circle(image, center, outer_radius, color, -1)\n",
        "    # Draw the inner circle (hole) with background color (0 for image, 0 for mask)\n",
        "    cv2.circle(image, center, inner_radius, 0, -1)\n",
        "\n",
        "def get_bounding_box(mask, class_id):\n",
        "    \"\"\"Calculates the bounding box for a given class_id in a mask.\"\"\"\n",
        "    y_indices, x_indices = np.where(mask == class_id)\n",
        "    if len(y_indices) == 0:\n",
        "        # Should not happen if draw was successful, but handle anyway\n",
        "        return 0, 0, 0, 0\n",
        "    xmin = int(np.min(x_indices))\n",
        "    xmax = int(np.max(x_indices))\n",
        "    ymin = int(np.min(y_indices))\n",
        "    ymax = int(np.max(y_indices))\n",
        "    # Add 1 to max coords because bbox max is exclusive in some contexts\n",
        "    # but often inclusive for definition. Let's use inclusive pixel coords.\n",
        "    return xmin, ymin, xmax, ymax\n",
        "\n",
        "def save_image(image_array, path):\n",
        "    \"\"\"Saves a float32 [0,1] image as uint8 [0,255] PNG.\"\"\"\n",
        "    image_uint8 = (image_array * 255).astype(np.uint8)\n",
        "    cv2.imwrite(path, image_uint8)\n",
        "\n",
        "def save_mask(mask_array, path):\n",
        "    \"\"\"Saves a uint8 mask directly.\"\"\"\n",
        "    cv2.imwrite(path, mask_array.astype(np.uint8))\n",
        "\n",
        "# --- Data Generation Function ---\n",
        "\n",
        "def generate_data_for_task(task_id, base_dir):\n",
        "    \"\"\"Generates the dataset for a specific task.\"\"\"\n",
        "    print(f\"--- Generating Task {task_id} ---\")\n",
        "    task_dir = os.path.join(base_dir, f\"task{task_id}\")\n",
        "    os.makedirs(task_dir, exist_ok=True)\n",
        "\n",
        "    for split, num_images in SPLITS.items():\n",
        "        print(f\"Generating {split} split ({num_images} images)...\")\n",
        "        split_dir = os.path.join(task_dir, split)\n",
        "        img_dir = os.path.join(split_dir, \"images\")\n",
        "        mask_dir = os.path.join(split_dir, \"masks\")\n",
        "        os.makedirs(img_dir, exist_ok=True)\n",
        "        os.makedirs(mask_dir, exist_ok=True)\n",
        "\n",
        "        annotations = []\n",
        "\n",
        "        # Use tqdm for progress bar\n",
        "        for i in tqdm(range(num_images), desc=f\"Task {task_id} - {split}\"):\n",
        "            img_filename = f\"img_{i:04d}.png\"\n",
        "            mask_filename = f\"mask_{i:04d}.png\"\n",
        "            img_path = os.path.join(img_dir, img_filename)\n",
        "            mask_path = os.path.join(mask_dir, mask_filename)\n",
        "\n",
        "            # Create base images\n",
        "            image = create_base_image(IMAGE_SIZE)\n",
        "            mask = create_base_image(IMAGE_SIZE).astype(np.uint8) # Mask uses int class IDs\n",
        "\n",
        "            # Determine object class and parameters based on task\n",
        "            if task_id == 1 or task_id == 2:\n",
        "                class_id = random.choice([1, 2]) # 1: Circle, 2: Donut\n",
        "                radius = CIRCLE_RADIUS_T12\n",
        "                outer_radius = DONUT_OUTER_RADIUS_T12\n",
        "                inner_radius = DONUT_INNER_RADIUS_T12\n",
        "                # Ensure object fits within image bounds\n",
        "                margin = max(radius, outer_radius) + 1\n",
        "                center_x = random.randint(margin, IMAGE_SIZE - margin -1)\n",
        "                center_y = random.randint(margin, IMAGE_SIZE - margin -1)\n",
        "                center = (center_x, center_y)\n",
        "\n",
        "                if class_id == 1: # Circle\n",
        "                    draw_circle(image, center, radius, OBJECT_INTENSITY)\n",
        "                    # Task 1: Mask is a circle\n",
        "                    # Task 2: Mask is a circle\n",
        "                    draw_circle(mask, center, radius, class_id) # Mask uses class_id as color\n",
        "                else: # Donut (Class 2)\n",
        "                    # Draw visual donut on image\n",
        "                    draw_donut(image, center, outer_radius, inner_radius, OBJECT_INTENSITY)\n",
        "                    if task_id == 1:\n",
        "                        # Task 1: Mask is donut shape\n",
        "                        draw_donut(mask, center, outer_radius, inner_radius, class_id)\n",
        "                    else: # Task 2\n",
        "                        # Task 2: Mask is a filled circle, even though image is donut\n",
        "                        draw_circle(mask, center, outer_radius, class_id)\n",
        "\n",
        "            elif task_id == 3:\n",
        "                class_id = random.choice([1, 2]) # 1: Small Circle, 2: Large Circle\n",
        "                radius = CIRCLE_RADIUS_T3_CLS1 if class_id == 1 else CIRCLE_RADIUS_T3_CLS2\n",
        "                # Ensure object fits within image bounds\n",
        "                margin = radius + 1\n",
        "                center_x = random.randint(margin, IMAGE_SIZE - margin -1)\n",
        "                center_y = random.randint(margin, IMAGE_SIZE - margin -1)\n",
        "                center = (center_x, center_y)\n",
        "\n",
        "                # Draw visual circle on image\n",
        "                draw_circle(image, center, radius, OBJECT_INTENSITY)\n",
        "                # Draw corresponding circle mask\n",
        "                draw_circle(mask, center, radius, class_id)\n",
        "\n",
        "            # Add noise to the image\n",
        "            noisy_image = add_noise(image)\n",
        "\n",
        "            # Get bounding box from the mask\n",
        "            xmin, ymin, xmax, ymax = get_bounding_box(mask, class_id)\n",
        "\n",
        "            # Save image and mask\n",
        "            save_image(noisy_image, img_path)\n",
        "            save_mask(mask, mask_path)\n",
        "\n",
        "            # Append annotation (use relative paths for portability)\n",
        "            relative_img_path = os.path.join(\"images\", img_filename)\n",
        "            relative_mask_path = os.path.join(\"masks\", mask_filename)\n",
        "            annotations.append([relative_img_path, relative_mask_path, xmin, ymin, xmax, ymax, class_id])\n",
        "\n",
        "        # Save annotations to CSV\n",
        "        annot_df = pd.DataFrame(annotations, columns=[\"image_path\", \"mask_path\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"class_id\"])\n",
        "        annot_path = os.path.join(split_dir, \"annotations.csv\")\n",
        "        annot_df.to_csv(annot_path, index=False)\n",
        "        print(f\"Saved annotations to {annot_path}\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the main directory to store the datasets\n",
        "    # In Colab, '/content/' is a good place for temporary storage\n",
        "    datasets_root_dir = \"/content/toy_datasets\"\n",
        "    os.makedirs(datasets_root_dir, exist_ok=True)\n",
        "\n",
        "    # Generate data for all three tasks\n",
        "    generate_data_for_task(1, datasets_root_dir) # Task 1: Distinguishing Shapes\n",
        "    generate_data_for_task(2, datasets_root_dir) # Task 2: Learning Patterns\n",
        "    generate_data_for_task(3, datasets_root_dir) # Task 3: Distinguishing Scales\n",
        "\n",
        "    print(\"\\n--- Dataset Generation Complete ---\")\n",
        "    print(f\"Datasets saved in: {datasets_root_dir}\")\n",
        "    print(\"Each task folder contains train/val/test splits.\")\n",
        "    print(\"Each split folder contains 'images', 'masks', and 'annotations.csv'.\")\n",
        "\n",
        "    # Example: Display structure of Task 1's train split\n",
        "    print(\"\\nExample structure for Task 1 train split:\")\n",
        "    !ls -lh {datasets_root_dir}/task1/train\n",
        "    print(\"\\nFirst 5 rows of Task 1 train annotations:\")\n",
        "    !head -n 6 {datasets_root_dir}/task1/train/annotations.csv\n",
        "\n",
        "    # Example: Display one generated image and mask from Task 1 train\n",
        "    import matplotlib.pyplot as plt\n",
        "    print(\"\\nExample Image and Mask (Task 1, Train, Image 0):\")\n",
        "    example_img_path = os.path.join(datasets_root_dir, \"task1/train/images/img_0000.png\")\n",
        "    example_mask_path = os.path.join(datasets_root_dir, \"task1/train/masks/mask_0000.png\")\n",
        "\n",
        "    if os.path.exists(example_img_path) and os.path.exists(example_mask_path):\n",
        "        img_display = cv2.imread(example_img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask_display = cv2.imread(example_mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "        axes[0].imshow(img_display, cmap='gray')\n",
        "        axes[0].set_title(\"Example Image\")\n",
        "        axes[0].axis('off')\n",
        "        axes[1].imshow(mask_display, cmap='gray', vmin=0, vmax=2) # Max value is 2 for class IDs\n",
        "        axes[1].set_title(\"Example Mask\")\n",
        "        axes[1].axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Could not find example image/mask to display.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision pandas opencv-python-headless tqdm matplotlib"
      ],
      "metadata": {
        "id": "VUl8O8nNZ2Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms.functional import to_tensor # Use functional transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# --- Custom PyTorch Dataset ---\n",
        "\n",
        "class ToyObjectDetectionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for the generated toy object detection data.\n",
        "    Loads images, masks, and annotations.\n",
        "    Outputs image tensor and a target dictionary compatible with TorchVision detection models.\n",
        "    \"\"\"\n",
        "    def __init__(self, split_dir, transforms=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            split_dir (str): Path to the specific split directory\n",
        "                             (e.g., '/content/toy_datasets/task1/train').\n",
        "            transforms (callable, optional): Optional transform to be applied\n",
        "                                             on a sample.\n",
        "        \"\"\"\n",
        "        self.split_dir = split_dir\n",
        "        self.img_dir = os.path.join(split_dir, \"images\")\n",
        "        self.mask_dir = os.path.join(split_dir, \"masks\")\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # Load annotations\n",
        "        annot_path = os.path.join(split_dir, \"annotations.csv\")\n",
        "        try:\n",
        "            self.annotations = pd.read_csv(annot_path)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: annotations.csv not found in {split_dir}\")\n",
        "            self.annotations = pd.DataFrame() # Empty dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fetches the sample (image, target) at the given index.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, target) where image is the image Tensor\n",
        "                   and target is a dictionary containing object detection information.\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # --- Load Data ---\n",
        "        annot_row = self.annotations.iloc[idx]\n",
        "        img_name = annot_row['image_path'] # Already relative like 'images/img_0000.png'\n",
        "        mask_name = annot_row['mask_path'] # Already relative like 'masks/mask_0000.png'\n",
        "\n",
        "        # Construct full paths\n",
        "        img_path = os.path.join(self.split_dir, img_name)\n",
        "        mask_path = os.path.join(self.split_dir, mask_name)\n",
        "\n",
        "        # Load image (OpenCV loads as BGR, convert to RGB)\n",
        "        # Load as float32 initially for potential transforms, normalize later\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "             raise FileNotFoundError(f\"Could not load image: {img_path}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Now HWC, RGB uint8\n",
        "\n",
        "        # Load mask (Grayscale)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) # HWC, Grayscale uint8\n",
        "        if mask is None:\n",
        "             raise FileNotFoundError(f\"Could not load mask: {mask_path}\")\n",
        "\n",
        "        # --- Extract Annotation Info ---\n",
        "        # Bounding box [xmin, ymin, xmax, ymax]\n",
        "        box = [annot_row['xmin'], annot_row['ymin'], annot_row['xmax'], annot_row['ymax']]\n",
        "        boxes = torch.as_tensor([box], dtype=torch.float32) # Shape: [1, 4]\n",
        "\n",
        "        # Label (Class ID) - ensure it's int64 for PyTorch criteria\n",
        "        # IMPORTANT: Often, class 0 is background. Our labels are 1 and 2.\n",
        "        # Ensure your model's final layer accounts for this or remap if needed.\n",
        "        label = annot_row['class_id']\n",
        "        labels = torch.as_tensor([label], dtype=torch.int64) # Shape: [1]\n",
        "\n",
        "        # Image ID\n",
        "        image_id = torch.tensor([idx])\n",
        "\n",
        "        # Area of bounding box\n",
        "        area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "\n",
        "        # Suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((1,), dtype=torch.int64) # Shape: [1]\n",
        "\n",
        "        # Segmentation Mask\n",
        "        # Extract the specific object's mask (pixels where mask == class_id)\n",
        "        # Convert to uint8 tensor, shape [1, H, W]\n",
        "        obj_mask = (mask == label).astype(np.uint8)\n",
        "        obj_masks = torch.as_tensor(obj_mask, dtype=torch.uint8).unsqueeze(0) # Add instance dim\n",
        "\n",
        "        # --- Create Target Dictionary ---\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = obj_masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        # --- Apply Transforms (if any) ---\n",
        "        # Convert image numpy HWC uint8 -> torch C H W float32 [0,1]\n",
        "        # Note: More complex transforms might need adjustment for target['boxes']/'masks'\n",
        "        image_tensor = to_tensor(image) # Converts to CxHxW and scales to [0.0, 1.0]\n",
        "\n",
        "        if self.transforms:\n",
        "             # Basic example: Applying transforms only to image tensor\n",
        "             # More advanced transforms might need to modify the target dict too\n",
        "             # Libraries like Albumentations handle this better for detection/segmentation\n",
        "             image_tensor = self.transforms(image_tensor)\n",
        "             # !! If transforms include resizing, bounding boxes and masks in target MUST be updated !!\n",
        "             # This basic example does not handle target transformation.\n",
        "\n",
        "\n",
        "        return image_tensor, target\n",
        "\n",
        "# --- Collate Function ---\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function for object detection.\n",
        "    Since targets can be dictionaries with tensors of varying sizes,\n",
        "    we don't stack them but keep them as a list. Images are stacked.\n",
        "\n",
        "    Args:\n",
        "        batch: A list of tuples, where each tuple is (image_tensor, target_dict).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (images, targets) where images is a stacked tensor of images\n",
        "               and targets is a list of target dictionaries.\n",
        "    \"\"\"\n",
        "    images = [item[0] for item in batch]\n",
        "    targets = [item[1] for item in batch]\n",
        "    images = torch.stack(images, dim=0) # Stack images along a new batch dimension\n",
        "    return images, targets\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Assume datasets are generated in /content/toy_datasets\n",
        "    datasets_root_dir = \"/content/toy_datasets\"\n",
        "    task_id = 1\n",
        "    split = 'train' # or 'val' or 'test'\n",
        "    target_split_dir = os.path.join(datasets_root_dir, f\"task{task_id}\", split)\n",
        "\n",
        "    # Check if the directory exists\n",
        "    if not os.path.exists(target_split_dir):\n",
        "        print(f\"Error: Dataset directory not found: {target_split_dir}\")\n",
        "        print(\"Please run the dataset generation script first.\")\n",
        "    else:\n",
        "        print(f\"Loading dataset from: {target_split_dir}\")\n",
        "\n",
        "        # 1. Create the Dataset instance\n",
        "        # We are not applying any further transforms here for simplicity\n",
        "        toy_dataset = ToyObjectDetectionDataset(split_dir=target_split_dir, transforms=None)\n",
        "\n",
        "        # Check dataset length\n",
        "        print(f\"Dataset size: {len(toy_dataset)} samples\")\n",
        "\n",
        "        # 2. Create the DataLoader instance\n",
        "        batch_size = 4\n",
        "        # Use num_workers > 0 for parallel data loading in real training\n",
        "        # Use pin_memory=True if using GPU for faster host-to-device transfers\n",
        "        data_loader = DataLoader(\n",
        "            toy_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True, # Shuffle for training\n",
        "            num_workers=0, # Set > 0 for parallel loading, but can cause issues in Colab sometimes\n",
        "            collate_fn=collate_fn,\n",
        "            pin_memory=False\n",
        "        )\n",
        "\n",
        "        # 3. Iterate through a batch\n",
        "        print(f\"\\nFetching one batch of size {batch_size}...\")\n",
        "        try:\n",
        "            images, targets = next(iter(data_loader))\n",
        "\n",
        "            # Print shapes and types\n",
        "            print(\"Batch loaded successfully!\")\n",
        "            print(f\"Images batch shape: {images.shape}\") # Should be [batch_size, C, H, W]\n",
        "            print(f\"Images batch dtype: {images.dtype}\")\n",
        "            print(f\"Number of targets in batch: {len(targets)}\") # Should be batch_size\n",
        "\n",
        "            # Inspect the first target dictionary in the batch\n",
        "            print(\"\\nInspecting target for the first sample in the batch:\")\n",
        "            first_target = targets[0]\n",
        "            for key, value in first_target.items():\n",
        "                 if isinstance(value, torch.Tensor):\n",
        "                     print(f\"  Target['{key}']: shape={value.shape}, dtype={value.dtype}\")\n",
        "                 else:\n",
        "                     print(f\"  Target['{key}']: {value}\")\n",
        "\n",
        "            # 4. Optional: Visualize the first image and its bounding box/mask\n",
        "            print(\"\\nVisualizing first image in batch...\")\n",
        "            img_to_show = images[0].permute(1, 2, 0).numpy() # Convert CHW -> HWC for plt\n",
        "            mask_to_show = targets[0]['masks'][0].numpy() # Get the first mask [H, W]\n",
        "            box_to_show = targets[0]['boxes'][0].numpy() # Get the first box [xmin, ymin, xmax, ymax]\n",
        "            label_to_show = targets[0]['labels'][0].item()\n",
        "\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "            # Image with BBox\n",
        "            axes[0].imshow(img_to_show)\n",
        "            rect = patches.Rectangle((box_to_show[0], box_to_show[1]), # bottom-left corner\n",
        "                                     box_to_show[2] - box_to_show[0], # width\n",
        "                                     box_to_show[3] - box_to_show[1], # height\n",
        "                                     linewidth=2, edgecolor='r', facecolor='none')\n",
        "            axes[0].add_patch(rect)\n",
        "            axes[0].set_title(f\"Image with BBox (Class {label_to_show})\")\n",
        "            axes[0].axis('off')\n",
        "\n",
        "            # Mask Overlay\n",
        "            axes[1].imshow(img_to_show)\n",
        "            axes[1].imshow(mask_to_show, cmap='jet', alpha=0.5) # Overlay mask\n",
        "            axes[1].set_title(\"Image with Mask Overlay\")\n",
        "            axes[1].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        except StopIteration:\n",
        "            print(\"DataLoader is empty (perhaps dataset is empty or batch size > dataset size).\")\n",
        "        except Exception as e:\n",
        "             print(f\"An error occurred while loading batch: {e}\")\n",
        "             import traceback\n",
        "             traceback.print_exc()"
      ],
      "metadata": {
        "id": "YBPJmb01Z3JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
        "from torchvision.models.detection.retinanet import RetinaNetHead\n",
        "from torchvision.ops import FeaturePyramidNetwork, sigmoid_focal_loss\n",
        "from torchvision.models._utils import IntermediateLayerGetter # To get intermediate ResNet layers\n",
        "from torchvision.models.detection.image_list import ImageList\n",
        "\n",
        "# --- Helper Modules ---\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"Basic Convolutional Block: Conv -> BN -> ReLU\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                              stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.bn(self.conv(x)))\n",
        "\n",
        "class UNetDecoderBlock(nn.Module):\n",
        "    \"\"\"Upsamples, concatenates with skip connection, and applies ConvBlocks\"\"\"\n",
        "    def __init__(self, in_channels_up, in_channels_skip, out_channels):\n",
        "        super().__init__()\n",
        "        # Upsampling uses bilinear interpolation followed by a 1x1 conv\n",
        "        # to potentially adjust channels before concatenation\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        # Adjust channels after upsampling to match the target out_channels / 2\n",
        "        # assuming we'll concatenate with skip connection of similar channel size\n",
        "        # Let's refine this: conv after upsampling to match skip connection expected size\n",
        "        # Or, more commonly, conv *after* concatenation.\n",
        "        # Let's upsample, then apply convs after concat.\n",
        "        # Total channels after concat = in_channels_up + in_channels_skip\n",
        "\n",
        "        self.conv1 = ConvBlock(in_channels_up + in_channels_skip, out_channels)\n",
        "        self.conv2 = ConvBlock(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x_up, x_skip):\n",
        "        x_up = self.upsample(x_up)\n",
        "        # Pad x_up if spatial dimensions don't match x_skip after upsampling\n",
        "        # This can happen due to pooling/padding in the encoder\n",
        "        diffY = x_skip.size()[2] - x_up.size()[2]\n",
        "        diffX = x_skip.size()[3] - x_up.size()[3]\n",
        "        x_up = F.pad(x_up, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        x = torch.cat([x_up, x_skip], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"Soft Dice Loss\"\"\"\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super().__init__()\n",
        "        self.smooth = smooth\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        # Apply softmax/sigmoid to logits to get probabilities\n",
        "        # Assuming multi-class segmentation, use Softmax\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "\n",
        "        # Flatten label and prediction tensors\n",
        "        # Select the probability of the target class for each pixel\n",
        "        # Assuming targets are LongTensor with class indices [B, H, W]\n",
        "        # Probs are FloatTensor [B, C, H, W]\n",
        "        targets_one_hot = F.one_hot(targets, num_classes=probs.shape[1]).permute(0, 3, 1, 2).float()\n",
        "        # targets_one_hot shape: [B, C, H, W]\n",
        "\n",
        "        probs_flat = probs.contiguous().view(probs.shape[0], probs.shape[1], -1)\n",
        "        targets_flat = targets_one_hot.contiguous().view(targets_one_hot.shape[0], targets_one_hot.shape[1], -1)\n",
        "\n",
        "        intersection = torch.sum(probs_flat * targets_flat, dim=2)\n",
        "        union = torch.sum(probs_flat, dim=2) + torch.sum(targets_flat, dim=2)\n",
        "\n",
        "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
        "\n",
        "        # Average dice across classes (ignoring background maybe?) and batch\n",
        "        # Let's average over all classes including background for now\n",
        "        dice_loss = 1.0 - dice.mean()\n",
        "        return dice_loss\n",
        "\n",
        "\n",
        "# --- Retina U-Net Model ---\n",
        "\n",
        "class RetinaUNet(nn.Module):\n",
        "    def __init__(self, num_classes_det, num_classes_seg,\n",
        "                 fpn_out_channels=256, backbone_name='resnet50', pretrained_backbone=True,\n",
        "                 # RetinaNet specific params (can tune these)\n",
        "                 anchor_sizes=((16,), (32,), (64,), (128,), (256,)), # Adjusted for P2-P5\n",
        "                 aspect_ratios=((0.5, 1.0, 2.0),) * 5,\n",
        "                 # Loss weights\n",
        "                 seg_loss_weight=1.0,\n",
        "                 # U-Net decoder channels\n",
        "                 unet_decoder_channels=(256, 128, 64), # Channels for P1, P0 blocks' outputs\n",
        "                 # ResNet layer names for skips (adjust if backbone changes)\n",
        "                 return_layers_map = {'layer1': 'C2', 'layer2': 'C3', 'layer3': 'C4', 'layer4': 'C5'},\n",
        "                 # Early ResNet layer for P0 skip connection\n",
        "                 skip_layer_p0 = 'relu' # Or 'conv1'\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes_det = num_classes_det # Including background for detector\n",
        "        self.num_classes_seg = num_classes_seg # Including background for segmentor\n",
        "        self.seg_loss_weight = seg_loss_weight\n",
        "\n",
        "        self.return_layers_map = return_layers_map\n",
        "\n",
        "\n",
        "        # --- Backbone (ResNet + Intermediate Layer Getter) ---\n",
        "        backbone = getattr(models, backbone_name)(pretrained=pretrained_backbone)\n",
        "\n",
        "        # We need C1 (for P1), C2, C3, C4, C5 features from the backbone\n",
        "        # And potentially an earlier layer for the P0 skip connection\n",
        "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers_map)\n",
        "\n",
        "        # Determine backbone output channels (standard for ResNet50)\n",
        "        # This might need adjustment for other backbones\n",
        "        # C1: 64 (after conv1/relu), C2: 256, C3: 512, C4: 1024, C5: 2048\n",
        "        # Let's get C1 from the initial conv layers if needed for P1 skip\n",
        "        self.early_features = nn.Sequential(\n",
        "            backbone.conv1,\n",
        "            backbone.bn1,\n",
        "            backbone.relu,\n",
        "            backbone.maxpool,\n",
        "        ) # Output C1 has 64 channels after relu or maxpool\n",
        "\n",
        "        backbone_out_channels = { # Channels *before* FPN\n",
        "            'C2': 256, 'C3': 512, 'C4': 1024, 'C5': 2048\n",
        "        }\n",
        "        c1_channels = 64 # After backbone.relu\n",
        "        early_skip_channels = 64 if skip_layer_p0 == 'relu' else 3 # If using input image\n",
        "\n",
        "        # --- FPN (P2-P5) ---\n",
        "        # FPN takes C2, C3, C4, C5 and outputs P2, P3, P4, P5\n",
        "        # Input channels must match backbone_out_channels for the specified layers\n",
        "        # FPN needs input layers names corresponding to the keys in backbone_out_channels\n",
        "        self.fpn = FeaturePyramidNetwork(\n",
        "            in_channels_list=[backbone_out_channels[k] for k in return_layers_map.values()],\n",
        "            out_channels=fpn_out_channels,\n",
        "            # We need P2 output as well, FPN usually starts from P3 based on C3\n",
        "            # Adjust FPN or use custom implementation if P2 is needed from C2 directly\n",
        "            # Torchvision FPN calculates Pk from Ck, so it should output P2,P3,P4,P5\n",
        "            # It also adds P6 by default from C5, we might not need it\n",
        "            extra_blocks=None # Disable P6 generation if not needed\n",
        "        )\n",
        "\n",
        "        # --- U-Net Decoder Extension (P2 -> P1 -> P0) ---\n",
        "        # P1 Block: Upsamples P2, concatenates with C1, outputs features for P1\n",
        "        self.decoder_p1 = UNetDecoderBlock(fpn_out_channels, c1_channels, unet_decoder_channels[0]) # P2->P1\n",
        "        # P0 Block: Upsamples P1, concatenates with early features, outputs features for P0\n",
        "        self.decoder_p0 = UNetDecoderBlock(unet_decoder_channels[0], early_skip_channels, unet_decoder_channels[1]) # P1->P0\n",
        "\n",
        "        # --- Segmentation Head ---\n",
        "        self.seg_head = nn.Conv2d(unet_decoder_channels[1], num_classes_seg, kernel_size=1)\n",
        "\n",
        "        # --- Detection Head ---\n",
        "        # Anchor Generator for P2, P3, P4, P5\n",
        "        # Need to know the strides of these features relative to input\n",
        "        # ResNet strides: C1=4, C2=4, C3=8, C4=16, C5=32\n",
        "        # FPN Strides: P2=4, P3=8, P4=16, P5=32\n",
        "        # Anchor sizes adjusted from paper (divided by 4 approx)\n",
        "        # Example: Original {32^2, 64^2, 128^2, 256^2, 512^2} -> {8^2, 16^2, 32^2, 64^2, 128^2}\n",
        "        # Paper uses {4^2, 8^2, 16^2, 32^2} for P2-P5 -> sizes (16, 32, 64, 128)\n",
        "        # Let's adjust default anchor sizes:\n",
        "        adjusted_anchor_sizes = ((16,), (32,), (64,), (128,)) # For P2, P3, P4, P5 respectively\n",
        "        aspect_ratios_per_level = (aspect_ratios[0],) * len(adjusted_anchor_sizes)\n",
        "\n",
        "        self.anchor_generator = AnchorGenerator(sizes=adjusted_anchor_sizes, aspect_ratios=aspect_ratios_per_level)\n",
        "\n",
        "        # RetinaNet Head (applies to P2, P3, P4, P5)\n",
        "        # Takes fpn_out_channels as input\n",
        "        self.det_head = RetinaNetHead(\n",
        "            in_channels=fpn_out_channels,\n",
        "            num_anchors=self.anchor_generator.num_anchors_per_location()[0],\n",
        "            num_classes=num_classes_det\n",
        "        )\n",
        "\n",
        "        # --- Loss Functions ---\n",
        "        self.seg_criterion_ce = nn.CrossEntropyLoss()\n",
        "        self.seg_criterion_dice = DiceLoss()\n",
        "        # Detection losses (Focal, SmoothL1) are handled inside RetinaNet logic usually\n",
        "        # We might need to implement the target assignment and loss calculation if not inheriting\n",
        "        # Let's prepare for external loss calculation for now.\n",
        "        # For Focal Loss, using torchvision.ops.sigmoid_focal_loss is efficient.\n",
        "        # For SmoothL1, use torch.nn.SmoothL1Loss or F.smooth_l1_loss.\n",
        "\n",
        "        # Need to handle target assignment for detection (match anchors to GT boxes)\n",
        "        # Torchvision's RetinaNet has self.compute_loss and self.proposal_matcher\n",
        "        # Replicating this is non-trivial. Let's assume for now we get assigned targets externally\n",
        "        # OR: We will need to integrate torchvision's loss computation logic.\n",
        "\n",
        "\n",
        "    def forward(self, images, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images (Tensor): Input images, shape [B, C, H, W]\n",
        "            targets (List[Dict[str, Tensor]], optional): Ground truth for training.\n",
        "                Each dict contains:\n",
        "                    'boxes' (Tensor[N, 4]): [xmin, ymin, xmax, ymax]\n",
        "                    'labels' (Tensor[N]): Integer labels (0 is often background)\n",
        "                    'masks' (Tensor[N, H, W]): Segmentation masks (uint8)\n",
        "                Required during training.\n",
        "\n",
        "        Returns:\n",
        "            During training (targets is not None):\n",
        "                Dict[str, Tensor]: Dictionary of losses.\n",
        "            During inference (targets is None):\n",
        "                List[Dict[str, Tensor]]: Detections per image.\n",
        "                Tensor: Segmentation map predictions [B, C_seg, H, W] (logits or probs).\n",
        "        \"\"\"\n",
        "        if isinstance(images, list):\n",
        "            original_image_sizes = [img.shape[-2:] for img in images]\n",
        "            images = torch.stack(images, 0) # Stack if input is a list\n",
        "        else: # Assuming input is already a tensor\n",
        "            original_image_sizes = [images.shape[-2:]] * images.shape[0]\n",
        "\n",
        "        # --- Backbone ---\n",
        "        # Get C1 features separately (used for P1 skip)\n",
        "        # Note: Depending on exactly where skip is needed, might adjust 'early_features'\n",
        "        # This gets features AFTER the first maxpool (stride 4)\n",
        "        c1_features = self.early_features(images) # Output e.g. 64 channels, stride 4\n",
        "\n",
        "        # Get C2, C3, C4, C5 features by running backbone on original images\n",
        "        # self.body uses IntermediateLayerGetter and expects original input\n",
        "        backbone_features = self.body(images)\n",
        "        # backbone_features is a dict {'C2': ..., 'C3': ..., 'C4': ..., 'C5': ...}\n",
        "\n",
        "        # --- FPN (P2-P5) ---\n",
        "        # Ensure the keys match what FPN expects\n",
        "        fpn_input_features = {k: backbone_features[k] for k in self.return_layers_map.values()}\n",
        "        fpn_features = self.fpn(fpn_input_features)\n",
        "        # fpn_features is an OrderedDict {'0': P2, '1': P3, '2': P4, '3': P5} (standard FPN keys)\n",
        "        p2 = fpn_features['C2'] # P2 corresponds to input C2\n",
        "        p3 = fpn_features['C3'] # P3 corresponds to input C3\n",
        "        p4 = fpn_features['C4'] # P4 corresponds to input C4\n",
        "        p5 = fpn_features['C5'] # P5 corresponds to input C5\n",
        "\n",
        "        fpn_outputs_for_detection = [p2, p3, p4, p5]\n",
        "\n",
        "        # --- U-Net Decoder (P2 -> P1 -> P0) ---\n",
        "        p1 = self.decoder_p1(p2, c1_features)\n",
        "        # For P0 skip connection, choose the appropriate early feature map\n",
        "        # Here using c1_features (output of early_features), adjust if needed\n",
        "        # e.g., if you need skip before maxpool, redefine early_features/c1_features\n",
        "        p0 = self.decoder_p0(p1, c1_features)\n",
        "\n",
        "        # --- Segmentation Head ---\n",
        "        seg_logits = self.seg_head(p0)\n",
        "        seg_logits = F.interpolate(seg_logits, size=original_image_sizes[0], mode='bilinear', align_corners=False)\n",
        "\n",
        "        # --- Detection Head ---\n",
        "        det_cls_logits, det_bbox_reg = self.det_head(fpn_outputs_for_detection)\n",
        "\n",
        "        # --- Create ImageList Object ---\n",
        "        # Convert list of [H, W] tensors/lists to list of tuples (H, W)\n",
        "        original_image_sizes_tuples = [tuple(s) for s in original_image_sizes]\n",
        "        image_list_obj = ImageList(images, original_image_sizes_tuples)\n",
        "\n",
        "\n",
        "        # --- Generate Anchors ---\n",
        "        # Pass the ImageList object to the anchor generator\n",
        "        anchors = self.anchor_generator(image_list_obj, fpn_outputs_for_detection)\n",
        "\n",
        "        # --- Output ---\n",
        "        losses = {}\n",
        "        detections = None\n",
        "\n",
        "        if self.training:\n",
        "            if targets is None:\n",
        "                raise ValueError(\"Targets must be provided during training.\")\n",
        "\n",
        "            # !!! ---> Placeholder for actual detection loss calculation <--- !!!\n",
        "            # Requires implementing or integrating anchor-target matching and loss computation\n",
        "            # Example: Use torchvision's built-in logic if possible, or implement manually\n",
        "            # gt_classes_target, gt_regression_target = self.assign_targets(anchors, targets) # Assign targets function needed\n",
        "            # loss_det_cls = self.compute_cls_loss(det_cls_logits, gt_classes_target) # e.g., Focal Loss\n",
        "            # loss_det_reg = self.compute_reg_loss(det_bbox_reg, gt_regression_target, gt_classes_target) # e.g., Smooth L1\n",
        "\n",
        "            # print(\"Warning: Detection loss calculation is currently a placeholder.\")\n",
        "            loss_det_cls = torch.tensor(0.0, device=images.device, requires_grad=True) # Dummy, ensure requires_grad for backprop test\n",
        "            loss_det_reg = torch.tensor(0.0, device=images.device, requires_grad=True) # Dummy\n",
        "            # !!! ---------------------------------------------------------- !!!\n",
        "\n",
        "            # --- Calculate Segmentation Loss ---\n",
        "            gt_seg_masks = []\n",
        "            for i, t in enumerate(targets):\n",
        "                h, w = original_image_sizes[i]\n",
        "                combined_mask = torch.zeros((h, w), dtype=torch.long, device=images.device)\n",
        "                if 'masks' in t and 'labels' in t and t['masks'].numel() > 0: # Check if masks/labels exist and are not empty\n",
        "                     instance_labels = t['labels']\n",
        "                     # Resize masks ONCE before the loop if possible\n",
        "                     instance_masks_resized = F.interpolate(t['masks'].unsqueeze(1).float(), size=(h,w)).squeeze(1).byte()\n",
        "                     for mask_idx in range(instance_masks_resized.shape[0]):\n",
        "                         label = instance_labels[mask_idx]\n",
        "                         mask = instance_masks_resized[mask_idx]\n",
        "                         combined_mask[mask > 0] = label\n",
        "                gt_seg_masks.append(combined_mask)\n",
        "            gt_seg_masks = torch.stack(gt_seg_masks, dim=0)\n",
        "\n",
        "            loss_seg_ce = self.seg_criterion_ce(seg_logits, gt_seg_masks)\n",
        "            loss_seg_dice = self.seg_criterion_dice(seg_logits, gt_seg_masks)\n",
        "            loss_segmentation = loss_seg_ce + loss_seg_dice\n",
        "\n",
        "            # --- Combine Losses ---\n",
        "            losses = {\n",
        "                \"loss_detector_cls\": loss_det_cls,\n",
        "                \"loss_detector_reg\": loss_det_reg,\n",
        "                \"loss_segmentation\": loss_segmentation * self.seg_loss_weight,\n",
        "            }\n",
        "            # Ensure total loss requires grad if components do\n",
        "            losses[\"total_loss\"] = loss_det_cls + loss_det_reg + losses[\"loss_segmentation\"]\n",
        "\n",
        "            return losses\n",
        "\n",
        "        else: # Inference mode\n",
        "            # !!! ---> Placeholder for actual detection post-processing <--- !!!\n",
        "            # Requires applying sigmoid, decoding boxes, NMS etc.\n",
        "            # print(\"Warning: Detection post-processing is currently a placeholder.\")\n",
        "            # Simulate output format\n",
        "            detections = [{\"boxes\": torch.empty((0, 4)), \"scores\": torch.empty((0,)), \"labels\": torch.empty((0,), dtype=torch.long)} for _ in range(images.shape[0])]\n",
        "            # !!! ---------------------------------------------------------- !!!\n",
        "\n",
        "            seg_predictions = F.softmax(seg_logits, dim=1)\n",
        "            return detections, seg_predictions\n",
        "\n",
        "\n",
        "# --- Example Instantiation ---\n",
        "if __name__ == '__main__':\n",
        "    # Example parameters (adjust based on your dataset)\n",
        "    NUM_DET_CLASSES = 3 # E.g., Background, Circle, Donut (or Benign, Malignant)\n",
        "    NUM_SEG_CLASSES = 3 # E.g., Background, Class1, Class2 (must match detector potentially)\n",
        "\n",
        "    model = RetinaUNet(\n",
        "        num_classes_det=NUM_DET_CLASSES,\n",
        "        num_classes_seg=NUM_SEG_CLASSES,\n",
        "        backbone_name='resnet50',\n",
        "        pretrained_backbone=True,\n",
        "        seg_loss_weight=1.0\n",
        "    )\n",
        "\n",
        "    model.eval() # Set to evaluation mode for inference example\n",
        "\n",
        "    # Create a dummy input batch\n",
        "    dummy_images = torch.randn(2, 3, 320, 320) # Batch size 2, 3 channels, 320x320\n",
        "\n",
        "    # --- Inference Example ---\n",
        "    with torch.no_grad():\n",
        "        detections, seg_predictions = model(dummy_images)\n",
        "\n",
        "    print(\"--- Inference Output ---\")\n",
        "    print(f\"Number of images with detections: {len(detections)}\")\n",
        "    if detections:\n",
        "         print(\"Detections for first image (placeholders):\", detections[0])\n",
        "    print(f\"Segmentation predictions shape: {seg_predictions.shape}\") # [B, C_seg, H, W]\n",
        "\n",
        "\n",
        "    # --- Training Example (Conceptual - Needs Targets & Full Loss Impl.) ---\n",
        "    model.train()\n",
        "    # Create dummy targets (replace with actual data loading)\n",
        "    dummy_targets = [\n",
        "        {\n",
        "            \"boxes\": torch.tensor([[50, 50, 100, 100], [150, 150, 180, 180]], dtype=torch.float32),\n",
        "            \"labels\": torch.tensor([1, 2], dtype=torch.int64), # Class 1, Class 2\n",
        "            \"masks\": torch.randint(0, 2, (2, 320, 320), dtype=torch.uint8) # Dummy instance masks\n",
        "        },\n",
        "        {\n",
        "            \"boxes\": torch.tensor([[70, 80, 120, 150]], dtype=torch.float32),\n",
        "            \"labels\": torch.tensor([1], dtype=torch.int64), # Class 1\n",
        "            \"masks\": torch.randint(0, 2, (1, 320, 320), dtype=torch.uint8)\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Note: This will likely fail or produce dummy losses until detection loss is implemented\n",
        "    try:\n",
        "        losses = model(dummy_images, dummy_targets)\n",
        "        print(\"\\n--- Training Output (Losses - Detection Loss Placeholder) ---\")\n",
        "        for k, v in losses.items():\n",
        "            print(f\"{k}: {v.item()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during training forward pass (expected if detection loss not fully implemented): {e}\")"
      ],
      "metadata": {
        "id": "2xnno93QcKHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.cuda.amp as amp # For mixed precision\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "from tqdm.notebook import tqdm # Use notebook version for Colab\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Assume these are defined in other files/cells ---\n",
        "# from dataset import ToyObjectDetectionDataset, collate_fn # Or paste the code here\n",
        "# from model import RetinaUNet # Or paste the code here\n",
        "# --- Paste the Dataset and Model code definitions here if not importing ---\n",
        "# <<< PASTE ToyObjectDetectionDataset and collate_fn HERE >>>\n",
        "# <<< PASTE RetinaUNet, UNetDecoderBlock, ConvBlock, DiceLoss HERE >>>\n",
        "# --- End of pasted code ---\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "DATASET_ROOT = \"/content/toy_datasets\"\n",
        "TASK_ID = 1 # Choose task 1, 2, or 3\n",
        "MODEL_SAVE_DIR = \"/content/retina_unet_checkpoints\"\n",
        "MODEL_SAVE_NAME = f\"retina_unet_task{TASK_ID}_best.pth\"\n",
        "\n",
        "# Training Hyperparameters (from paper where specified)\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 20 # As per paper for 2D\n",
        "NUM_EPOCHS = 50 # Adjust as needed\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# For detection classes, add 1 for the background class\n",
        "NUM_DET_CLASSES_MODEL = 3 # BG, Class1, Class2 (for toy dataset)\n",
        "# For segmentation classes, assuming same mapping\n",
        "NUM_SEG_CLASSES_MODEL = 3 # BG, Class1, Class2\n",
        "\n",
        "# Mixed Precision Training (recommended for larger models/GPUs)\n",
        "USE_AMP = torch.cuda.is_available()\n",
        "\n",
        "# --- Ensure save directory exists ---\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# --- Training Function ---\n",
        "\n",
        "def train_model(model, dataloaders, optimizer, num_epochs, model_save_path):\n",
        "    \"\"\"\n",
        "    Main training loop for Retina U-Net.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The RetinaUNet model instance.\n",
        "        dataloaders (dict): Dictionary containing 'train' and 'val' DataLoaders.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer instance.\n",
        "        num_epochs (int): Number of epochs to train for.\n",
        "        model_save_path (str): Path to save the best model checkpoint.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    # Using validation loss as proxy for best model since mAP requires implemented post-processing\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # AMP Scaler\n",
        "    scaler = amp.GradScaler(enabled=USE_AMP)\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': [],\n",
        "               'train_loss_seg': [], 'val_loss_seg': [],\n",
        "               'train_loss_det_cls': [], 'val_loss_det_cls': [],\n",
        "               'train_loss_det_reg': [], 'val_loss_det_reg': []}\n",
        "\n",
        "    print(f\"Starting training on {DEVICE}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_loss_seg = 0.0\n",
        "            running_loss_det_cls = 0.0\n",
        "            running_loss_det_reg = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            progress_bar = tqdm(dataloaders[phase], desc=f\"{phase.capitalize()} Epoch {epoch+1}\")\n",
        "            for images, targets in progress_bar:\n",
        "                images = images.to(DEVICE)\n",
        "                # Move targets to device (list of dicts with tensors)\n",
        "                targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                # Track history only in train phase\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # AMP context manager\n",
        "                    with amp.autocast(enabled=USE_AMP):\n",
        "                        # Model returns dict of losses in train mode\n",
        "                        # In eval mode, it returns detections, seg_preds\n",
        "                        # BUT: We need losses for validation monitoring too.\n",
        "                        # Let's assume the model can optionally return losses in eval mode\n",
        "                        # or we re-run in train mode without gradients for val loss.\n",
        "                        # Easiest: Modify model forward to always compute losses if targets given.\n",
        "                        # Let's assume current model calculates losses if targets are present.\n",
        "                        loss_dict_or_outputs = model(images, targets)\n",
        "\n",
        "                        if isinstance(loss_dict_or_outputs, dict): # Training or Eval with targets\n",
        "                           loss_dict = loss_dict_or_outputs\n",
        "                           # Check if all expected losses are present\n",
        "                           if not all(k in loss_dict for k in [\"total_loss\", \"loss_segmentation\", \"loss_detector_cls\", \"loss_detector_reg\"]):\n",
        "                               print(f\"Warning: Loss dict missing keys: {loss_dict.keys()}\")\n",
        "                               # Handle missing keys gracefully (e.g., assign 0)\n",
        "                               total_loss = loss_dict.get(\"total_loss\", torch.tensor(0.0, device=DEVICE))\n",
        "                               loss_seg = loss_dict.get(\"loss_segmentation\", torch.tensor(0.0, device=DEVICE))\n",
        "                               loss_det_cls = loss_dict.get(\"loss_detector_cls\", torch.tensor(0.0, device=DEVICE))\n",
        "                               loss_det_reg = loss_dict.get(\"loss_detector_reg\", torch.tensor(0.0, device=DEVICE))\n",
        "                           else:\n",
        "                               total_loss = loss_dict[\"total_loss\"]\n",
        "                               loss_seg = loss_dict[\"loss_segmentation\"]\n",
        "                               loss_det_cls = loss_dict[\"loss_detector_cls\"]\n",
        "                               loss_det_reg = loss_dict[\"loss_detector_reg\"]\n",
        "                        else:\n",
        "                            # Handle case where model returns predictions in eval mode\n",
        "                            # Cannot calculate loss directly here without targets/loss logic\n",
        "                            print(\"Warning: Model in eval mode did not return loss dict.\")\n",
        "                            total_loss = torch.tensor(0.0, device=DEVICE) # Assign dummy loss\n",
        "                            loss_seg, loss_det_cls, loss_det_reg = total_loss, total_loss, total_loss\n",
        "\n",
        "\n",
        "                    # Backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        # Check if total_loss is valid before backward pass\n",
        "                        if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
        "                           print(f\"Warning: Invalid loss detected: {total_loss.item()}. Skipping batch.\")\n",
        "                           continue # Skip this batch\n",
        "\n",
        "                        scaler.scale(total_loss).backward()\n",
        "                        # Gradient Clipping (optional, but can help stability)\n",
        "                        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += total_loss.item() * images.size(0)\n",
        "                # Check if loss components are tensors before calling .item()\n",
        "                running_loss_seg += loss_seg.item() * images.size(0) if torch.is_tensor(loss_seg) else loss_seg * images.size(0)\n",
        "                running_loss_det_cls += loss_det_cls.item() * images.size(0) if torch.is_tensor(loss_det_cls) else loss_det_cls * images.size(0)\n",
        "                running_loss_det_reg += loss_det_reg.item() * images.size(0) if torch.is_tensor(loss_det_reg) else loss_det_reg * images.size(0)\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix(loss=total_loss.item())\n",
        "\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_loss_seg = running_loss_seg / len(dataloaders[phase].dataset)\n",
        "            epoch_loss_det_cls = running_loss_det_cls / len(dataloaders[phase].dataset)\n",
        "            epoch_loss_det_reg = running_loss_det_reg / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print(f'{phase.capitalize()} Total Loss: {epoch_loss:.4f}')\n",
        "            print(f'  Seg Loss: {epoch_loss_seg:.4f} | Det Cls Loss: {epoch_loss_det_cls:.4f} | Det Reg Loss: {epoch_loss_det_reg:.4f}')\n",
        "\n",
        "            history[f'{phase}_loss'].append(epoch_loss)\n",
        "            history[f'{phase}_loss_seg'].append(epoch_loss_seg)\n",
        "            history[f'{phase}_loss_det_cls'].append(epoch_loss_det_cls)\n",
        "            history[f'{phase}_loss_det_reg'].append(epoch_loss_det_reg)\n",
        "\n",
        "\n",
        "            # Save the best model based on validation loss\n",
        "            if phase == 'val' and epoch_loss < best_val_loss:\n",
        "                print(f\"Validation loss improved ({best_val_loss:.4f} --> {epoch_loss:.4f}). Saving model...\")\n",
        "                best_val_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - start_time\n",
        "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best Validation Loss: {best_val_loss:4f}')\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Prepare Datasets and DataLoaders\n",
        "    print(\"Loading datasets...\")\n",
        "    try:\n",
        "        train_split_dir = os.path.join(DATASET_ROOT, f\"task{TASK_ID}\", \"train\")\n",
        "        val_split_dir = os.path.join(DATASET_ROOT, f\"task{TASK_ID}\", \"val\")\n",
        "\n",
        "        # Add data augmentation here if needed (e.g., using torchvision.transforms or albumentations)\n",
        "        # train_transforms = ...\n",
        "        # val_transforms = ...\n",
        "\n",
        "        train_dataset = ToyObjectDetectionDataset(split_dir=train_split_dir, transforms=None)\n",
        "        val_dataset = ToyObjectDetectionDataset(split_dir=val_split_dir, transforms=None)\n",
        "\n",
        "        dataloaders = {\n",
        "            'train': DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=2, collate_fn=collate_fn, pin_memory=True),\n",
        "            'val': DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             num_workers=2, collate_fn=collate_fn, pin_memory=True)\n",
        "        }\n",
        "        print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "        print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading datasets: {e}\")\n",
        "        print(\"Please ensure the dataset generation script ran successfully and paths are correct.\")\n",
        "        exit()\n",
        "\n",
        "    # 2. Initialize Model\n",
        "    print(\"\\nInitializing model...\")\n",
        "    # Note: Pass num_classes including background for detector head\n",
        "    model = RetinaUNet(\n",
        "        num_classes_det=NUM_DET_CLASSES_MODEL,\n",
        "        num_classes_seg=NUM_SEG_CLASSES_MODEL,\n",
        "        # Add other RetinaUNet parameters if needed (backbone, etc.)\n",
        "    )\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    # 3. Initialize Optimizer (Adam as per paper)\n",
        "    # Filter parameters that require gradients (useful if freezing backbone layers)\n",
        "    params_to_update = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = optim.Adam(params_to_update, lr=LEARNING_RATE)\n",
        "\n",
        "    # 4. Run Training\n",
        "    print(\"\\nStarting training loop...\")\n",
        "    print(\"--- WARNING: Detection loss/post-processing in model are placeholders! ---\")\n",
        "    print(\"--- Training/Validation relies on these being correctly implemented. ---\")\n",
        "    print(\"--- Validation tracks LOSS, not mAP, due to placeholders. ---\")\n",
        "\n",
        "    model_best, history = train_model(\n",
        "        model,\n",
        "        dataloaders,\n",
        "        optimizer,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        model_save_path=os.path.join(MODEL_SAVE_DIR, MODEL_SAVE_NAME)\n",
        "    )\n",
        "\n",
        "    # 5. Plot Training History (Losses)\n",
        "    print(\"\\nPlotting training history...\")\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Total Loss')\n",
        "    plt.plot(history['val_loss'], label='Val Total Loss')\n",
        "    plt.title('Total Loss vs. Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_loss_seg'], label='Train Seg Loss')\n",
        "    plt.plot(history['val_loss_seg'], label='Val Seg Loss')\n",
        "    plt.plot(history['train_loss_det_cls'], label='Train Det Cls Loss (PH)') # PH=Placeholder\n",
        "    plt.plot(history['val_loss_det_cls'], label='Val Det Cls Loss (PH)')\n",
        "    plt.plot(history['train_loss_det_reg'], label='Train Det Reg Loss (PH)')\n",
        "    plt.plot(history['val_loss_det_reg'], label='Val Det Reg Loss (PH)')\n",
        "    plt.title('Component Losses vs. Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Training finished.\")"
      ],
      "metadata": {
        "id": "vmoE6Fw-dz3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yWusW76yfvzf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}